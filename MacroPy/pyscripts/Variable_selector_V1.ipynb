{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h2o\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def info(self, msg):\n",
    "        print 'INFO: {0}'.format(msg)\n",
    "\n",
    "\n",
    "class VariableSelector():\n",
    "    #Currently only support's H2o and LightGBM for generating Feature Importance\n",
    "    #either input the pandas dataframe using dataset or specify CSV location\n",
    "    #the Specify addtional categories  are the columns which need to be treated as Category\n",
    "    def __init__(self, Algorithm = \"LightGBM\",dataset = \"\", Input_Dir = \"\", Target =\"\",\n",
    "                 Exclude_columns = \"\", Num_features = 20, preprocess = True,log = None,\n",
    "                 spcfy_cat = []):\n",
    "        self.Algorithm = Algorithm\n",
    "        self.dataset = dataset\n",
    "        self.data = self.load_data(dataset,Input_Dir)\n",
    "        self.Input_Dir = Input_Dir\n",
    "        self.log = self.setup_log(log)\n",
    "        self.log.info('Run initiated.')\n",
    "        self.Target =Target\n",
    "        self.Exclude_columns = Exclude_columns\n",
    "        self.spcfy_cat = spcfy_cat\n",
    "        self.cols_to_use = self.get_cols_to_use(self.data,Target,Exclude_columns)\n",
    "        self.Cat_columns = self.get_Cat_columns(self.data,Target,spcfy_cat,self.cols_to_use)\n",
    "        self.variable_importance  = self.get_variable_importance(Algorithm,self.data,Target,Input_Dir,\n",
    "                                                                 Exclude_columns,preprocess,self.cols_to_use,\n",
    "                                                                 spcfy_cat,self.Cat_columns)\n",
    "        \n",
    "    def get_cols_to_use(self,data,Target,Exclude_columns):\n",
    "        if isinstance(Exclude_columns, str):\n",
    "            Exclude_columns = list(Exclude_columns)  \n",
    "        return list(set(data.columns)-set([Target])- set(Exclude_columns))\n",
    "    \n",
    "    def get_Cat_columns(self,data,Target,spcfy_cat,cols_to_use):\n",
    "        return list(set(data[cols_to_use].select_dtypes(include=['object']).columns)\n",
    "                                         .union(set(spcfy_cat)))\n",
    "    \n",
    "    \n",
    "    def get_variable_importance(self,Algorithm,dataset,Target,Input_Dir,\n",
    "                                Exclude_columns,preprocess,cols_to_use,\n",
    "                                spcfy_cat,Cat_columns):\n",
    "        data = self.load_data(dataset,Input_Dir)\n",
    "        self.log.info('Dataset Loaded')   \n",
    "        data = self.preprocess_data(preprocess,self.Algorithm,data,spcfy_cat,self.Cat_columns) \n",
    "        self.data = data\n",
    "        return self.build_model(data,Algorithm,Target,cols_to_use,Cat_columns)\n",
    "\n",
    "        \n",
    "    def build_model(self,data,Algorithm,Target,cols_to_use,Cat_columns):\n",
    "        if Algorithm in [\"H2o\"]:\n",
    "            model = H2ORandomForestEstimator(model_id=\"Random_forest_FI\",\n",
    "                                            ntrees=400,\n",
    "                                            stopping_rounds=2,\n",
    "                                            score_each_iteration=True,\n",
    "                                            seed=1000000)\n",
    "            model.train(cols_to_use,y= Target, training_frame=data)\n",
    "            self.log.info('Model Build')  \n",
    "            return model._model_json['output']['variable_importances'].as_data_frame()\n",
    "        else:\n",
    "            train_data = lgb.Dataset( data[cols_to_use],data[Target] ,\n",
    "                                     #feature_name = cols_to_use\n",
    "             #                        ,categorical_feature=Cat_columns\n",
    "                                    )\n",
    "            # train_data.set_categorical_feature(Cat_columns)\n",
    "            self.log.info(train_data) \n",
    "            params = {\n",
    "                            'task': 'train',\n",
    "                            'boosting_type': 'gbdt',\n",
    "                            'objective': 'regression',\n",
    "                            #'metric': {'l2', 'auc'},\n",
    "                            #'num_leaves': 31,\n",
    "                            'learning_rate': 0.1,\n",
    "                            'feature_fraction': 0.9,\n",
    "                            'bagging_fraction': 0.8,\n",
    "                            'bagging_freq': 5,\n",
    "                            'verbose': 0,\n",
    "                           # 'categorical_feature' : ['name:' + str(col) for col in Cat_columns]\n",
    "                        }\n",
    "            if len(data[Target].unique()) == 2:\n",
    "                params[\"objective\"] = 'binary'\n",
    "            elif len(data[Target].unique()) < 10 :\n",
    "                params[\"objective\"] = 'multiclass'\n",
    "            model = lgb.train(params,\n",
    "                            train_data,\n",
    "                            num_boost_round=200,\n",
    "                           # valid_sets=lgb_eval,\n",
    "                           # early_stopping_rounds=25\n",
    "                             )\n",
    "            self.log.info(params) \n",
    "            self.log.info('Model Build')  \n",
    "            importances = model.feature_importance()\n",
    "            #print (importances.shape)\n",
    "            imp_list = []\n",
    "            for row in zip(data[cols_to_use].columns, map(lambda x:round(x,4), importances)):\n",
    "                imp_list.append(row)\n",
    "            return (pd.DataFrame(imp_list, columns=['Column', 'Importance'])).sort_values(['Importance'], ascending = False)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def setup_log(self, log):\n",
    "        if log is None:\n",
    "            log = Logger()\n",
    "        return log\n",
    "    \n",
    "    def load_data(self,data,Input_Dir):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            return data\n",
    "        else:\n",
    "            return pd.read_csv(Input_Dir)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def preprocess_data(self,preprocess,Algorithm,data,spcfy_cat,Cat_columns):\n",
    "        if preprocess == False:\n",
    "            self.log.info(\"No Preprocessing Done\")\n",
    "        elif  Algorithm in [\"H2o\"]:\n",
    "            h2o.init()\n",
    "            h2o.remove_all() \n",
    "            self.log.info('Preprocessed Data h2o frame') \n",
    "            data = h2o.H2OFrame(data)\n",
    "            for col in spcfy_cat:\n",
    "                data[col] = data[col].asfactor()\n",
    "\n",
    "            return data\n",
    "              \n",
    "        else:\n",
    "            for c in Cat_columns:\n",
    "                lbl = LabelEncoder()\n",
    "                lbl.fit(list(data[c].values) )\n",
    "                data[c] = lbl.transform(list(data[c].values))\n",
    "                self.log.info('Preprocessed Data pd frame')\n",
    "            return data\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Run initiated.\n",
      "INFO: Dataset Loaded\n",
      "INFO: Preprocessed Data pd frame\n",
      "INFO: <lightgbm.basic.Dataset object at 0x7f3f6cb24990>\n",
      "INFO: {'categorical_column': [], 'task': 'train', 'verbose': 0, 'max_bin': 255, 'objective': 'regression', 'bagging_freq': 5, 'learning_rate': 0.1, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'boosting_type': 'gbdt'}\n",
      "INFO: Model Build\n",
      "   Column  Importance\n",
      "0    disp         0.0\n",
      "1    drat         0.0\n",
      "2      vs         0.0\n",
      "3    gear         0.0\n",
      "4     mpg         0.0\n",
      "5    qsec         0.0\n",
      "6    Cars         0.0\n",
      "7      am         0.0\n",
      "8      wt         0.0\n",
      "9    carb         0.0\n",
      "10    cyl         0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    m = VariableSelector(Input_Dir = \"/home/puneetj/CLA_Macro/MacroPy/data/mtcars.csv\",\n",
    "                     Algorithm = \"LightGBM\",\n",
    "                    # dataset = data,\n",
    "                     Target =\"hp\",\n",
    "                     preprocess = True,\n",
    "                 #    Exclude_columns = [\"status_sex\"],\n",
    "                  #  spcfy_cat = [\"installment_rate_income\"]\n",
    "                    )\n",
    "    print(m.variable_importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
